# AMERICAN SIGN LANGUAGE TRANSLATOR
---
### **Contents**
- [Introduction](#Introduction)
- [Preview](#Preview)
- [Run](#Run)
- [Demo](#Demo)
- [Inference](#Inference)
- [Training](#Training)
- [Accuracy measures](#Accuracy-measures)
- [Dataset used](#Dataset-used)
- [Other Models Tried](#Other-Models-Tried)
- [Documentation](#Documentation)
- [References](#References)

### **Introduction**
In this presentation, we will be demonstrating a Computer Vision demo using YOLOv5 as our final model on the American Sign Language Dataset including 26 classes.The model identifies signs in real time as well as with input image or audio and builds bounding boxes showing label with confidence value..The model is showcased using streamlit which can take input as an image.

---

### **Preview**

Sign Language Detection Using YOLO Algorithm

<img width="400" height="400" src="https://github.com/Tanwish2003/ASL_Translator/assets/101427402/ccd7cef6-ee9d-435f-b4d8-34c3149ea654">

<img width="400" height="400" src="https://github.com/Tanwish2003/ASL_Translator/assets/101427402/5b761b25-0fed-4bc6-be1b-e20c0f874865">


**Yolo model:**
You Only Look Once (YOLO) family of detection frameworks aim to build a real time object detector, which what they lack in small differences of accuracy when compared to the two stage detectors, are able to provide faster inferences.It predicts over limited bounding boxes generated by splitting image into a grid of cells, with each cell being responsible for classification and generation of bounding boxes, which are then consolidated by NMS.

**YOLOv5 architecture:**

![alt text](https://blog.roboflow.com/content/images/2020/06/image-11.png)


[Follow the link and input image or video. It will return the file with signs identified along with the confidence value.]: #

 
---
### **Demo**

CLICK ON IMAGE BELOW TO WATCH DEMO

[<img src="https://th.bing.com/th/id/OIP.QVfbOcyK8ExX_XVXHaSwPwHaDe?rs=1&pid=ImgDetMain" width="600" height="300"
/>](https://youtube.com/shorts/_72cppYpmA8?feature=share)


### **Installation**

<details open>
<summary>Install using following commands</summary>

Clone repo and install [requirements.txt](https://github.com/Tanwish2003/ASL_Translator) in a
[**Python>=3.7.0**](https://www.python.org/) environment, including
[**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).

```bash
git clone https://github.com/Tanwish2003/ASL_Translator  # clone
cd temp
pip install -r .\requirements.txt # install
pip install streamlit==1.29.0     # install
pip install imageio               # install

```

</details>

---
### **Run**
Clone the repo in virtual environment and open the website using the following command:
```bash
python -m streamlit run app.py
```

<details open>
<summary>Inference</summary>


- You can run inference inside YoloV5_main folder by using this command:
 

 ```bash
 python .\app.py --source    0        --weights exp3\weights\best.pt
 python .\app.py --source    img.jpg  --weights exp3\weights\best.pt    # image
 python .\app.py --source    vid.mp4  --weights exp3\weights\best.pt    # video
 python .\app.py --source    path/    --weights exp3\weights\best.pt    # directory
 python .\app.py --source    path/*.jpg  --weights exp3\weights\best.pt                       # glob
 python .\app.py --source    'https://youtu.be/Zgi9g1ksQHc'  --weights exp3\weights\best.pt   # YouTube
 python .\app.py --source    'rtsp://example.com/media.mp4'  --weights exp3\weights\best.pt   # RTSP, RTMP, HTTP stream
 ```
- The results are saved to `runs/detect`
- if it dosent work with forward slash, use backward slash

</details>

---
### **Training**
Tensorboard results:
<img width="800" src="https://github.com/Tanwish2003/ASL_Translator/assets/101427402/d353e5f1-a41a-41b9-8bd9-b0099cd3f14a">
https://github.com/Tanwish2003/ASL_Translator/temp/exp3/results.csv
for details on each epoch

---

### **Accuracy measures**

On testing our model on our testing dataset it returned the following metrics
- Precision:0.983
- recall:0.987
- F1Score:0.985
- mAP@.5:0.993
- mAP@.5:.95: 0.84
- Accuracy: 0.989
- for class wise detail refer this:
https://docs.google.com/spreadsheets/d/1vNaIl_QhOzDW6Ybr08uVx-mlAI5BWqOOxjPJnY_aVR8/edit#gid=0
---


### **Dataset-used**
We have used an American Sign Language Dataset consisting of 3399 images having 26 American Sign language alphabets. This dataset was used because of the availability of annotations which were required for training our YOLO-v5s model.
* [American Sign Language](https://universe.roboflow.com/sign-language/american-language/dataset/1)

---
### **Other Models Tried**

-  [![Open All Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Okbh6gQZ3KLBaAOWbp21hX4RaC6cakjF?usp=drive_link) (Model using CNN layers for real time sign language detection))


-  [![Open All Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DP17cpMxvbPOzlNeaU4AO-6lO9uHHU_7)(Model uses holistic mediapipeline)


----

### **Documentation**

See the [Sign language translator doc](https://docs.google.com/document/d/1iM9WTKpYz_AFDq3MxQTOzACmkR6sv78Tvk742qjx724/edit?usp=sharing) for full documentation on implementation of models and deployment.


---

### **References**

* https://github.com/ultralytics/yolov5
* https://github.com/roboflow-ai
* https://zone.biblio.laurentian.ca/bitstream/10219/3843/1/Thesis%20FINAL%20-%20Devina%20Vaghasiya%20-%2025-Mar-2021.pdf
*  https://docs.ovh.com/asia/en/publiccloud/ai/training/web-service-yolov5/


***Thanks for Reading***

---


<!-- 
https://user-images.githubusercontent.com/107802412/184129250-b72b0c76-0256-49f3-b9e9-6106d6437897.mp4



https://user-images.githubusercontent.com/107802412/184130019-2472c959-ec44-4211-91a5-9ce838633cf3.mp4 -->





